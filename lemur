#!/usr/bin/env python3
#
# Created on 11/07/2023 by Nick Sapoval

import datetime
import argparse
import os
import logging
import sys
import signal
import time
import shutil
import subprocess as subp
from itertools import repeat
from multiprocessing import Pool

import pandas as pd
import numpy as np
import pysam


__version__ = "1.0.1"


def CTRLChandler(signum, frame):
    global SIGINT
    SIGINT = True
    sys.stderr.write("Caught request to terminate (CTRL+C), exiting...\n")
    sys.exit(128)

signal.signal(signal.SIGINT, CTRLChandler)


class LemurRunEnv():
    GENES = ['RpsE', 'RpsG', 'RpoA', 'RplK', 'RpsQ', 'RplA', 'TsaD', 'RpsK', 'LeuS', 'RpsS',
            'RpsM', 'Ffh', 'RplO', 'SecY', 'ArgS', 'CysS', 'PheS', 'RplB', 'RpsB', 'RplP',
            'RplV', 'RpsO', 'Gtp1', 'RpoB', 'RplD', 'RpsL', 'RplC', 'RpsI', 'RpsD', 'RplM',
            'SerS', 'RplR', 'ValS', 'FtsY', 'RplF', 'RpsH', 'RplN', 'HisS', 'RpsC', 'RplE']
    CIGAR_OPS_CHAR = ["M", "I", "D", "=", "X", "H", "S"]
    CIGAR_OPS_INT  = [ 0,   1,   2,   7,   8,   5,   4 ]
    TAXONOMY_RANKS = ['species', 'genus', 'family', 'order', 'class', 'phylum', 'clade', 'superkingdom']
    FIXED_COSTS    = {0: 1.,
                      1: 0.005,
                      2: 0.005,
                      7: 1.,
                      8: 0.01,
                      4: 0.05,
                      5: 0.001}


    def __init__(self):
        self.CURRDATE = datetime.date.today().strftime('%Y-%m-%d')
        self.CURRTIME = datetime.date.today().strftime('%H:%M:%S')

        self.args = self.parse_args()
        self.logger = self.logging_setup()

        self.aln_score = self.args.aln_score
        self.by_gene = self.args.aln_score_gene

        if self.args.sam_input:
            self.sam_path = self.args.sam_input
        else:
            self.sam_path = self.args.output + "/reads.sam"

        if os.path.exists(self.args.output):
            self.log(f"Output directory {self.args.output} exists! Results will be overwritten...", logging.WARNING)
            shutil.rmtree(self.args.output)
        os.makedirs(self.args.output)

        self.tsv_output_prefix = self.args.output + "/relative_abundance"

        self.lli_threshold = 0.01
        self.low_abundance_threshold = 0.0001

        self.rank = self.args.rank


    def parse_args(self):
        parser = argparse.ArgumentParser(description="""
            Lemur example:
            lemur -i <input> -o <output_dir> -t <threads>
            """)
        parser.add_argument('-v', '--version', action='version', version='%(prog)s ' + __version__)
        
        main_args = parser.add_argument_group(title="Main Mob arguments")
        main_args.add_argument(
            "-i",
            "--input",
            type=str,
            help="Input FASTQ file for the analysis",
            required=True
        )
        main_args.add_argument(
            "-o",
            "--output",
            type=str,
            default=f"mob_out_{self.CURRDATE}_{self.CURRTIME}/",
            help="Folder where the Mob output will be stored"
        )
        main_args.add_argument(
            "-d",
            "--db-prefix",
            type=str,
            help="Path to the folder with individual Emu DBs for each marker gene"
        )
        main_args.add_argument(
            "--tax-path",
            type=str,
            help="Path to the taxonomy.tsv file (common for all DBs)"
        )
        main_args.add_argument(
            "-t",
            "--num-threads",
            type=int,
            default=20,
            help="Number of threads you want to use"
        )
        main_args.add_argument(
            "--aln-score",
            '--aln-score', choices=['AS', 'edit', 'markov'], default='AS',
            help='AS: Use SAM AS tag for score, edit: Use edit-type distribution for score, markov: Score CIGAR as markov chain',
            type=str
        )
        main_args.add_argument(
            "--aln-score-gene",
            action="store_true"
        )
        main_args.add_argument(
            "-r",
            "--rank",
            type=str,
            default="species",
        )
        main_args.add_argument(
            "--min-aln-len-ratio",
            type=float,
            default=0.75
        )
        main_args.add_argument(
            "--min-fidelity",
            type=float,
            default=0.50
        )
        main_args.add_argument(
            "--ref-weight",
            type=float,
            default=1.
        )
        #  Minimap2 specific arguments
        mm2_args = parser.add_argument_group(title="minimap2 arguments")
        mm2_args.add_argument(
            '--mm2-N', type=int, default=50,
            help='minimap max number of secondary alignments per read [50]'
        )
        mm2_args.add_argument(
            '--mm2-K', type=int, default=500000000,
            help='minibatch size for minimap2 mapping [500M]'
        )
        mm2_args.add_argument(
            '--mm2-type', choices=['map-ont', 'map-hifi', 'map-pb', 'sr'], default='map-ont',
            help='ONT: map-ont [map-ont], PacBio (hifi): map-hifi, PacBio (CLR): map-pb, short-read: sr'
        )
        # Verbosity/logging/additional info
        misc_args = parser.add_argument_group(title="Miscellaneous arguments")
        misc_args.add_argument(
            "--keep-alignments",
            action="store_true",
            help="Keep SAM files after the mapping (might require a lot of disk space)"
        )
        misc_args.add_argument(
            "-e",
            "--log-file",
            type=str,
            default="stdout",
            help="File for logging [default: stdout]"
        )
        misc_args.add_argument(
            "--sam-input",
            type=str,
        )
        misc_args.add_argument(
            "--verbose",
            action='store_true',
            help="Enable DEBUG level logging"
        )
        misc_args.add_argument(
            "--save-intermediate-profile",
            action="store_true",
            help="Will save abundance profile at every EM step"
        )
        misc_args.add_argument(
            "--width-filter",
            action="store_true",
            help="Apply width filter"
        )
        misc_args.add_argument(
            "--gid-name",
            action="store_true"
        )

        return parser.parse_args()


    def init_taxonomy(self):
        self.df_taxonomy = pd.read_csv(self.args.tax_path, sep='\t', index_col='tax_id', dtype=str)
        self.log(self.df_taxonomy)
        self.log(f"Loaded taxonomy file {self.args.tax_path}")


    def init_F(self):
        '''Initializes the frequency vector F which represents the relative abundance of every species in the
        reference database.'''
        self.F = pd.Series(data=[1. / len(self.df_taxonomy)] * len(self.df_taxonomy), 
                           index=self.df_taxonomy.index,
                           name="F")
        self.log(f"Initialized abundance vector")


    def run_minimap2(self):
        '''
        Runs minimap2 with the following command:

            minimap2 -ax map-ont -N 50 -p .9 -f 0 --sam-hit-only --eqx -t <n_threads> <db>/species_taxid.fasta \
                <input> -o <sam_path>

        '''
        db_sequence_file = os.path.join(self.args.db_prefix, 'species_taxid.fasta')

        cmd_str = f"minimap2 \
                            -ax {self.args.mm2_type} \
                            -N {self.args.mm2_N} \
                            -K {self.args.mm2_K} \
                            -p .9 \
                            -f 0 \
                            --sam-hit-only \
                            --eqx \
                            -t {self.args.num_threads} \
                            {db_sequence_file} \
                            {self.args.input} \
                            -o {self.sam_path}"

        p = subp.Popen(cmd_str, 
                    shell=True,
                    stdin=None,
                    stdout=subp.PIPE,
                    stderr=subp.PIPE,
                    close_fds=True,
                    text=True)
        fstdout, fstderr = p.communicate()
        rc = p.returncode

        if (rc != 0):
            self.log(f"minimap2 failed executing:\n{cmd_str}\n\nSTDOUT:{fstdout}\n\nSTDERR:{fstderr}", logging.CRITICAL)
            sys.exit(rc)
        else:
            self.log(fstdout, logging.DEBUG)
            self.log(fstderr, logging.DEBUG)


    def build_alignment_model(self):
        if self.args.aln_score == "AS":
            return 
        elif self.args.aln_score == "markov":
            self.build_transition_mats()
        elif self.args.aln_score == "edit":
            self.edit_cigar = self.build_edit_cost()
        else:
            self.fixed_cigar = {0: 1.,
                                1: 0.005,
                                2: 0.005,
                                7: 1.,
                                8: 0.01,
                                4: 0.05,
                                5: 0.01}


    def build_edit_cost(self):
        '''
        Constructs a dictionary object with the operation costs for each value in the CIGAR string depending on which
        cost model to use (given at command-line).

        Keys to the output dictionary are integer indices of the CIGAR characters. Namely they are from the set
        {0, 1, 2, 7, 8, 4, 5} where: 0=Match, 1=Insertion, 2=Deletion, 7=Id, 8=Mismatch, 4=Hard Clip, 5=Soft Clip.

        Returns:
            op_costs (dict):    Dict of the form { <Op_Num>: <Cost> }
        '''
        op_costs = {0: 0.,  # Match
                    1: 0.,  # Ins
                    2: 0.,  # Del
                    7: 0.,  # Id
                    8: 0.,  # X
                    4: 0.,  # Hard clip
                    5: 0.}  # Soft clip
        ops_w_cost = [1, 2, 8, 4, 5]

        if self.by_gene:
            pass
        else:
            total = 0
            cigars = self.extract_cigars_all(self.sam_path)
            for cigar in cigars:
                for t in cigar:
                    OP, L = t
                    if OP in ops_w_cost:
                        op_costs[OP] += L
                        total += L
            
            for OP in op_costs:
                if OP in ops_w_cost:
                    op_costs[OP] = op_costs[OP] / total
                else:
                    op_costs[OP] = 1.
            
            return op_costs


    def build_transition_mats(self):
        if self.by_gene:
            gene_cigars = self.extract_cigars_per_gene(self.sam_path)
            self.gene_transition_mats = {self.GENES[i]: mat 
                                         for i, mat in enumerate(map(
                                             self.build_transition_mat, gene_cigars.values()
                                             ))
                                        }
        else:
            cigars = self.extract_cigars_all(self.sam_path)
            self.transition_mat = self.build_transition_mat(cigars)


    def extract_cigars_per_gene(self, sam_path):
        samfile = pysam.AlignmentFile(sam_path)
        
        gene_cigars = {gene: [] for gene in self.GENES}
        for read in samfile.fetch():
            if not read.is_secondary and not read.is_supplementary:
                gene_cigars[read.reference_name.split(":")[1].split("/")[-1]].append(read.cigartuples)

        return gene_cigars


    def extract_cigars_all(self, sam_path):
        samfile = pysam.AlignmentFile(sam_path)
        
        cigars = []
        for read in samfile.fetch():
            if not read.is_secondary and not read.is_supplementary:
                cigars.append(read.cigartuples)

        return cigars


    def build_transition_mat(self, cigars):
        transition_mat = pd.DataFrame(data=[[0] * (len(self.CIGAR_OPS_INT) + 1) \
                                            for _ in range(len(self.CIGAR_OPS_INT) + 1)], 
                                      index=self.CIGAR_OPS_INT + [-1], 
                                      columns=self.CIGAR_OPS_INT + [-1])

        for cigar in cigars:
            N = len(cigar)
            prev_OP = None
            for i, t in enumerate(cigar):
                OP, L = t
                if not ((i == 0 or i == N - 1) and (OP == self.CIGAR_OPS_INT[-1])):
                    if OP in self.CIGAR_OPS_INT:
                        transition_mat.at[OP, OP] += L - 1
                        if not prev_OP is None:
                            transition_mat.at[prev_OP, OP] += 1
                        prev_OP = OP
            # transition_mat.at[prev_OP, -1] += 1

        transition_mat = transition_mat.div(transition_mat.sum(axis=1), axis=0).fillna(0.)
        self.log(transition_mat, logging.DEBUG)
        
        return transition_mat


    @staticmethod
    def __get_aln_len(aln):
        '''Gets the alignment length from a tuple of the CIGAR stats from the .get_cigar_stats() function.'''
        _, I, _, _, _, _, _, E, X, _, _ = aln.get_cigar_stats()[0]
        return I + E + X


    def build_P_rgs_df(self):
        '''
        Builds a pandas data-frame containing the Likelihood values of the Read-Given-referenceSequence (hence `rgs`).

        This function creates two dataframes that are stored in the parent object, one called `P_rgs_df` containing
        the Likelihood values by Read-Target-Mapping, and another called `gene_stats_df' containing gene-wise
        summary stats over these Likelihoods, which are then mapped onto the Likelihood dataframe.

        How exactly the Likelihood is calculated depends on the command line inputs, but if `aln_score == "AS"`
        is True then the calculation is simply:
            log_P = log( Minimap2_AlignmentScore / [2 * Alignment-Length ])

        Returns:
            self.P_rgs_df (pandas.DataFrame):   Dataframe with fields ("Read_ID", "Target_ID", "log_P", "Gene",
                                                "Reference", "aln_len")
        '''
        samfile = pysam.AlignmentFile(self.sam_path)

        # The fields in the result are all pulled from the SAM file directly **except** for the `log_P` field.
        P_rgs_data = {"Read_ID": [], "Target_ID": [], "log_P": [], "cigar": [], "Gene": [], "Reference": [], "aln_len": []}
        #               <sam>           <sam>          (calc)       <sam>        <sam>       <sam>            <sam>

        for i, aln in enumerate(samfile.fetch()):
            qname = aln.query_name
            aln_score = aln.get_tag("AS")
            if aln_score > 0:
                if self.args.gid_name:
                    species_tid = aln.reference_name.rsplit("_", maxsplit=1)[0]
                else:
                    species_tid = int(aln.reference_name.split(":")[0])
                    gene = aln.reference_name.split(":")[1].split("/")[-1]
                cigar = aln.cigartuples

                if self.args.aln_score == "AS":
                    log_P_score = np.log(aln_score / (2 * self.__get_aln_len(aln)))
                    P_rgs_data["log_P"].append(log_P_score)

                P_rgs_data["Read_ID"].append(qname)
                P_rgs_data["Target_ID"].append(species_tid)
                if not self.args.aln_score == "AS":
                    P_rgs_data["cigar"].append(cigar)
                P_rgs_data["Gene"].append(gene)
                P_rgs_data["Reference"].append(aln.reference_name)
                P_rgs_data["aln_len"].append(self.__get_aln_len(aln))

            if (i + 1)% 100000 == 0:
                self.log(f"build_P_rgs_df extracted {i+1} reads", logging.DEBUG)


        if not self.args.aln_score == "AS":
            with Pool(self.args.num_threads) as pool:
                if self.aln_score == "markov":
                    if self.by_gene:
                        cigar_gene_mat_tuples = [(cigar, self.gene_transition_mats[P_rgs_data["Gene"][i]]) \
                                                 for i, cigar in enumerate(P_rgs_data["cigar"])]
                        P_rgs_data["log_P"] = pool.starmap(self.score_cigar_markov,
                                                                       cigar_gene_mat_tuples)
                    else:
                        P_rgs_data["log_P"] = pool.starmap(self.score_cigar_markov,
                                                                       zip(P_rgs_data["cigar"],
                                                                       repeat(self.transition_mat)))
                elif self.aln_score == "edit":
                    if self.by_gene:
                        log_P_func = self.score_cigar_fixed(P_rgs_data["cigar"][i], self.gene_edit_cigars[gene])
                    else:
                        P_rgs_data["log_P"] = pool.starmap(self.score_cigar_fixed,
                                                                       zip(P_rgs_data["cigar"],
                                                                       repeat(self.edit_cigar)))
                else:
                    if self.by_gene:
                        log_P_func = self.score_cigar_fixed(P_rgs_data["cigar"][i], self.gene_fixed_cigars[gene])
                    else:
                        P_rgs_data["log_P"] = pool.starmap(self.score_cigar_fixed,
                                                                       zip(P_rgs_data["cigar"],
                                                                       repeat(self.fixed_cigar)))

        del P_rgs_data["cigar"]

        self.P_rgs_df = pd.DataFrame(data=P_rgs_data)
        self.P_rgs_df["max_aln_len"] = self.P_rgs_df.groupby("Read_ID")["aln_len"].transform('max')
        self.P_rgs_df["log_P"] = self.P_rgs_df["log_P"] * self.P_rgs_df["max_aln_len"] / self.P_rgs_df["aln_len"]
        self.P_rgs_df["max_log_P"] = self.P_rgs_df.groupby("Read_ID")["log_P"].transform('max')

        self.P_rgs_df.to_csv(f"{self.args.output}/P_rgs_df_raw.tsv", sep='\t', index=False)

        # Get gene-wise stats from the file `gene2len.tsv` and read them onto this dataframe...
        self.gene_stats_df = pd.read_csv(f"{self.args.db_prefix}/gene2len.tsv", sep='\t')
        self.gene_stats_df["type"] = self.gene_stats_df["#id"].str.split("_").str[-1].str.split(":").str[0]
        self.gene_stats_df["Target_ID"] = self.gene_stats_df["#id"].str.split(":").str[0].astype(int)
        gene_p_rgs_len_df = self.P_rgs_df.merge(self.gene_stats_df,
                                                how="left", right_on="#id", left_on="Reference")
        gene_p_rgs_len_df["aln_len_ratio"] = gene_p_rgs_len_df["aln_len"] / gene_p_rgs_len_df["length"]
        gene_p_rgs_len_df["fidelity"] = gene_p_rgs_len_df["log_P"] / gene_p_rgs_len_df["aln_len"]

        gene_p_rgs_len_df["ref_len_weighted_log_P"] = gene_p_rgs_len_df["log_P"] + \
                                                      self.args.ref_weight * np.log(gene_p_rgs_len_df["aln_len_ratio"])
        gene_p_rgs_len_df.to_csv(f"{self.args.output}/gene_P_rgs_df_raw.tsv", sep='\t', index=False)
        if self.args.ref_weight != 0:
            self.P_rgs_df["log_P"] = gene_p_rgs_len_df["ref_len_weighted_log_P"]

        if self.args.aln_score == "AS":
            self.P_rgs_df = self.P_rgs_df[(gene_p_rgs_len_df["aln_len_ratio"] >= self.args.min_aln_len_ratio) &
                                          (gene_p_rgs_len_df["log_P"]>=1.1*gene_p_rgs_len_df["max_log_P"]) & 
                                          (gene_p_rgs_len_df["log_P"]>=np.log(self.args.min_fidelity))]
            gene_p_rgs_len_df = gene_p_rgs_len_df[(gene_p_rgs_len_df["aln_len_ratio"] >= self.args.min_aln_len_ratio) &
                                                  (gene_p_rgs_len_df["log_P"]>=1.1*gene_p_rgs_len_df["max_log_P"]) & 
                                                  (gene_p_rgs_len_df["log_P"]>=np.log(self.args.min_fidelity))]
        else:
            self.P_rgs_df = self.P_rgs_df[(gene_p_rgs_len_df["aln_len_ratio"] >= self.args.min_aln_len_ratio) &
                                          (gene_p_rgs_len_df["fidelity"] >= self.args.min_fidelity)]
            gene_p_rgs_len_df = gene_p_rgs_len_df[(gene_p_rgs_len_df["aln_len_ratio"] >= self.args.min_aln_len_ratio) &
                                                  (gene_p_rgs_len_df["fidelity"] >= self.args.min_fidelity)]

        ref2genome_df = pd.read_csv(f"{self.args.db_prefix}/reference2genome.tsv", sep='\t', 
                                    names=["Name", "Genome"])
        self.P_rgs_df = self.P_rgs_df.merge(ref2genome_df, how="left", left_on="Reference", right_on="Name")

        self.P_rgs_df = self.P_rgs_df.groupby(by=["Read_ID", "Target_ID"]).max()
        __df_id_count = self.P_rgs_df.reset_index().groupby("Read_ID").count().reset_index()[["Read_ID", "Target_ID"]]
        __df_id_count.columns = ["Read_ID", "Map_Count"]
        __df = self.P_rgs_df.reset_index().merge(__df_id_count, how="left", on="Read_ID")
        self.unique_mapping_support_tids = set(__df[__df["Map_Count"]==1]["Target_ID"])
        self.log(self.P_rgs_df)
        self.P_rgs_df.reset_index().to_csv(f"{self.args.output}/P_rgs_df.tsv", sep='\t', index=False)
        return self.P_rgs_df


    def logging_setup(self):
        logger = logging.getLogger("Mob")
        logger.setLevel(logging.DEBUG)
        
        if self.args.log_file == "stdout":
            handler = logging.StreamHandler(stream=sys.stdout)
        elif self.args.log_file == "stderr":
            handler = logging.StreamHandler(stream=sys.stderr)
        else:
            handler = logging.FileHandler(self.args.log_file, mode='w')
        
        error_handler = logging.StreamHandler(stream=sys.stderr)
        error_handler.setLevel(logging.ERROR)

        if self.args.verbose:
            handler.setLevel(logging.DEBUG)
        else:
            handler.setLevel(logging.INFO)

        formatter = logging.Formatter("%(asctime)s %(message)s",
                                      datefmt="%Y-%m-%d %I:%M:%S %p")
        handler.setFormatter(formatter)
        error_handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.addHandler(error_handler)
        return logger


    def log(self, msg, level=logging.DEBUG):
        if level == logging.DEBUG:
            self.logger.debug(f"DEBUG:\t{msg}")
        elif level == logging.INFO:
            self.logger.info(f"INFO:\t{msg}")
        elif level == logging.WARNING:
            self.logger.warning(f"WARNING:\t{msg}")
        elif level == logging.ERROR:
            self.logger.error(f"ERROR:\t{msg}")
        elif level == logging.CRITICAL:
            self.logger.critical(f"CRITICAL:\t{msg}")
    

    @staticmethod
    def score_cigar_markov(cigar, T):
        N = len(cigar)
        prev_OP = None
        log_P = 0.
        for i, t in enumerate(cigar):
            OP, L = t
            if not ((i == 0 or i == N - 1) and (OP == LemurRunEnv.CIGAR_OPS_INT[-1])):
                if OP in [1, 2, 7, 8, 5, 4]:
                    if T.at[OP, OP] != 0:
                        log_P += (L - 1) * np.log(T.at[OP, OP])
                    else:
                        log_P += (L - 1) * np.log(LemurRunEnv.FIXED_COSTS[OP])
                    
                    if not prev_OP is None:
                        if T.at[prev_OP, OP] != 0:
                            log_P += np.log(T.at[prev_OP, OP])

                    prev_OP = OP
                
        # log_P += np.log(T.at[prev_OP, -1])
        return log_P


    @staticmethod
    def score_cigar_fixed(cigar, fixed_op_cost):
        N = len(cigar)
        aln_cigar = [ct for i, ct in enumerate(cigar)] # if not ((i == 0 or i == N - 1) and (ct[0] == LemurRunEnv.CIGAR_OPS_INT[-1]))]
        log_P = sum(map(lambda OP_L: np.log(fixed_op_cost[OP_L[0]]) * OP_L[1], aln_cigar))
        return log_P


    @staticmethod
    def logSumExp(ns):
        '''
        Computes log(e^x1+e^x2+...) for very small x values that might run into a numerical stability problem if
        calculated the regular way. Specifically this function subtracts a constant from every number first, then
        does the calculation, then adds the constant back at the end.

        Args:
            ns (np.array): array of arguments to be log-sum-exp'ed
        '''
        # TODO: maybe remove code duplication with the external version of this function
        __max = np.max(ns)
        if not np.isfinite(__max):
            __max = 0
        ds = ns - __max
        with np.errstate(divide='ignore'):
            sumOfExp = np.exp(ds).sum()
        return __max + np.log(sumOfExp)


    def EM_step(self, final=False):
        '''
        Function to run a single step of the EM algorithm.

        The EM setup for this model is basically the same as in the document from Alex for the nano16s project:
            E-Step:     P(t|r) := [ P(r|t)*F(t) ] / sum_{t_in_Taxa} [P(r|t)*F(t)]
            M-Step:     F(t)   := sum_{r_in_Reads} [P(t|r)]

        '''
        # t0 = datetime.datetime.now().timestamp()
        if final:
            self.F = self.final_F

        # 1) Merges the current frequency vector into the Likelihood df, creating a new df called
        #   P_tgr, i.e. P( target | read )
        self.P_tgr = self.P_rgs_df.reset_index().merge(self.F,
                                                       how="inner", 
                                                       left_on="Target_ID", 
                                                       right_index=True)        #Step 0: Merge-F-to_LL
        # t1 = datetime.datetime.now().timestamp()

        # 2) Compute Likelihood x Prior:
        self.P_tgr["P(r|t)*F(t)"] = self.P_tgr.log_P + np.log(self.P_tgr.F)     #Step 1: Calc_LL*F
        # t2 = datetime.datetime.now().timestamp()

        # 3) Compute Lik*Pri Scale factors by Read:
        self.P_tgr_sum = EM_get_Prgt_Ft_sums(self.P_tgr, self.args.num_threads)
        # t3 = datetime.datetime.now().timestamp()

        # 4) Read in per-read scale factors to (read,target)-level dataframe
        self.P_tgr = self.P_tgr.merge(self.P_tgr_sum,
                                      how="left",
                                      left_on="Read_ID",
                                      right_index=True,
                                      suffixes=["", "_sum"])                            #Step 3: Merge-LL*Fsum-to-LL
        # t4 = datetime.datetime.now().timestamp()

        # 5) E-Step: Calculate P(t|r) = [ P(r|t)*F(t) / sum_{t in taxo} (P(r|t)*F(t)) ]
        self.P_tgr["P(t|r)"] = self.P_tgr["P(r|t)*F(t)"] - self.P_tgr["P(r|t)*F(t)_sum"]    #Step 4: Calc-Ptgr
        # t5 = datetime.datetime.now().timestamp()
        self.log(set(self.P_tgr["Target_ID"]), logging.DEBUG)
        n_reads = len(self.P_tgr_sum)

        # 6) M-Step: Update the estimated values of F(t) = sum_{r}[P(t|r)]                     #Step 5: Recalc F
        self.F = self.P_tgr[["Target_ID", "P(t|r)"]].groupby("Target_ID") \
                                                    .agg(lambda x: np.exp(LemurRunEnv.logSumExp(x) - np.log(n_reads)))["P(t|r)"]
        self.F.name = "F"
        self.F = self.F.loc[self.F!=0]

        # Logging: report the sum of the F vector (which should be 1.0)
        self.log(self.F.sum(), logging.DEBUG)
        # t6 = datetime.datetime.now().timestamp()

        if final:
            self.final_F = self.F

    def compute_loglikelihood(self):
        '''Computes the total loglikelihood of the model, which should increase at every iteration or something
        is wrong.'''
        return self.P_tgr["P(r|t)*F(t)_sum"].sum()
    

    def EM_complete(self):
        '''Workhorse function that runs the EM-algorithm one step at a time and determines when the convergence
        criteria has been met, stopping once it has.'''
        n_reads = len(set(self.P_rgs_df.reset_index()["Read_ID"]))
        if n_reads == 0:
            self.log(f"No alignments available for EM algorithm.\nVerify SAM file, {self.self.args.output}P_rgs_df_raw.tsv, and consider lowering --min-aln-len-ratio or --fidelity flags.", logging.ERROR)
            exit(1)
        else:
            self.low_abundance_threshold = 1. / n_reads

        if self.args.width_filter:
            __P_rgs_df = self.P_rgs_df.reset_index()
            tids = list(self.F.index)
            filter_pass = []
            for tid in tids:
                tid_df = __P_rgs_df[__P_rgs_df["Target_ID"] == tid]
                N_genes_hit = len(set(tid_df["Gene"]))
                N_genes = len(set(self.gene_stats_df[self.gene_stats_df["Target_ID"] == tid]["type"]))
                N_reads = len(set(tid_df["Read_ID"]))
                if N_reads > 10:
                    E_N_genes_hit, variance = LemurRunEnv.get_expected_gene_hits(N_genes, N_reads)
                        # For small number of reads mapped, i.e. 4-8 ratio is helpful since 6 out of 8 works
                    if (N_genes_hit / E_N_genes_hit > 0.7 or \
                        # For "large" numbers like 20 MGs at 49 reads we can check by variance; formally as reads -> infinity, the variance is more informative
                        E_N_genes_hit - N_genes_hit <= 3 * variance) \
                        and N_genes_hit > 1:
                        filter_pass.append(True)
                    else:
                        filter_pass.append(False)
                else:
                    if N_reads == 0:
                        filter_pass.append(False)
                    else:
                        filter_pass.append(True)
            self.F = self.F.loc[filter_pass]

        lli = -np.inf
        i = 1
        while True:
            self.log(f"Starting EM iteration {i}", logging.DEBUG)
            self.EM_step()
            new_lli = self.compute_loglikelihood()

            lli_delta = new_lli - lli
            self.log(f"Current log likelihood {new_lli}", logging.DEBUG)
            self.log(f"LLI delta is {lli_delta}", logging.DEBUG)
            lli = new_lli

            self.first_EM_df = self.df_taxonomy.merge(self.F, how='right', left_index=True, right_index=True).reset_index()

            if self.args.save_intermediate_profile:
                intermediate_df = self.df_taxonomy.merge(self.F, how='right', left_index=True, right_index=True).reset_index()
                intermediate_df.to_csv(f"{self.tsv_output_prefix}-EM-{i}.tsv", sep='\t', index=False)

            if lli_delta < self.lli_threshold:
                self.log(f"Low abundance threshold: {self.low_abundance_threshold:.8f}", logging.INFO)
                # self.final_F = self.F.loc[(self.F>=self.low_abundance_threshold) &
                #                           (self.F.index.isin(self.unique_mapping_support_tids))]

                # 02/07/2024
                # __P_rgs_df = self.P_rgs_df.reset_index()
                # tids = list(self.F.index)
                # filter_pass = []
                # for tid in tids:
                #     tid_df = __P_rgs_df[__P_rgs_df["Target_ID"] == tid]
                #     N_genes_hit = len(set(tid_df["Gene"]))
                #     N_genes = len(set(self.gene_stats_df[self.gene_stats_df["Target_ID"] == tid]["type"]))
                #     N_reads = len(set(tid_df["Read_ID"]))
                #     E_N_genes_hit, variance = LemurRunEnv.get_expected_gene_hits(N_genes, N_reads)
                #         # For small number of reads mapped, i.e. 4-8 ratio is helpful since 6 out of 8 works
                #     if (N_genes_hit / E_N_genes_hit > 0.7 or \
                #         # For "large" numbers like 20 MGs at 49 reads we can check by variance; formally as reads -> infinity, the variance is more informative
                #         E_N_genes_hit - N_genes_hit <= 3 * variance) \
                #         and N_genes_hit > 1:
                #         filter_pass.append(True)
                #     else:
                #         filter_pass.append(False)
                # self.final_F = self.F.loc[filter_pass & (self.F>=self.low_abundance_threshold)]
                self.final_F = self.F.loc[self.F>=self.low_abundance_threshold]
                # ----------

                self.EM_step(final=True)

                self.freq_to_lineage_df()
                self.collapse_rank()

                return

            i += 1


    @staticmethod
    def get_expected_gene_hits(N_genes, N_reads):
        '''
        Given the number of genes being used (g) and the number of reads (r), gets the mean/variance for the
        number of genes with non-zero read-hits assuming each read hits each gene with uniform probability 1/g.

        Let p=1/g (i.e., the uniform probabilty). The formulas for the output are:
            E = g[1-(1-p)^r]
            V = g(1-p)^r + g^2 [(1-p)(1-2p)^r - (1-p)^2r]
        '''
        E = N_genes * (1 - np.power(1 - 1 / N_genes, N_reads)) 
        V = N_genes * np.power(1 - 1 / N_genes, N_reads) + \
            N_genes * N_genes * (1 - 1 / N_genes) * np.power(1 - 2 / N_genes, N_reads) - \
            N_genes * N_genes * np.power((1 - 1 / N_genes), 2 * N_reads)
        return E, V


    def freq_to_lineage_df(self):
        results_df = self.df_taxonomy.merge(self.final_F, how='right', left_index=True, right_index=True).reset_index()
        results_df.to_csv(f"{self.tsv_output_prefix}.tsv", sep='\t', index=False)
        
        return results_df


    def collapse_rank(self):
        df_emu = pd.read_csv(f"{self.tsv_output_prefix}.tsv", sep='\t')
        if self.rank not in self.TAXONOMY_RANKS:
            raise ValueError("Specified rank must be in list: {}".format(self.TAXONOMY_RANKS))
        keep_ranks = self.TAXONOMY_RANKS[self.TAXONOMY_RANKS.index(self.rank):]
        for keep_rank in keep_ranks:
            if keep_rank not in df_emu.columns:
                keep_ranks.remove(keep_rank)
        if "estimated counts" in df_emu.columns:
            df_emu_copy = df_emu[['F', 'estimated counts'] + keep_ranks]
            df_emu_copy = df_emu_copy.replace({'-': 0})
            df_emu_copy = df_emu_copy.astype({'F': 'float', 'estimated counts': 'float'})
        else:
            df_emu_copy = df_emu[['F'] + keep_ranks]
            df_emu_copy = df_emu_copy.replace({'-': 0})
            df_emu_copy = df_emu_copy.astype({'F': 'float'})
        df_emu_copy = df_emu_copy.groupby(keep_ranks, dropna=False).sum()
        output_path = f"{self.tsv_output_prefix}-{self.rank}.tsv"
        df_emu_copy.to_csv(output_path, sep='\t')
        self.log(df_emu_copy.nlargest(30, ["F"]), logging.DEBUG)
        self.log(f"File generated: {output_path}\n", logging.DEBUG)

def logSumExp_ReadId(readid, ns):
    '''
    This is a duplicate of the logSumExp function in the LemurRunEnv object, except that it accepts and
    returns the Read ID as an argument. This is so that we can parallelize this function and still associate
    the results with the Read ID after being returned.
    '''
    __max = np.max(ns)
    if not np.isfinite(__max):
        __max = 0
    ds = ns - __max
    with np.errstate(divide='ignore'):
        sumOfExp = np.exp(ds).sum()
    return readid, __max + np.log(sumOfExp)

def EM_get_Prgt_Ft_sums(P_tgr, nthreads):
    '''
    Computes the log-sum-exp and does the sum aggregation to the read level efficiently.

    This function is a strictly numpy replacement for the pandas *.groupby(...).agg(self.logSumExp)
    function that was the bottleneck in the EM_step() function. Basically it takes the DF self.P_tgr
    and returns a 2-column data frame with columns ["Read_ID","P(r|t)*F(t)"] where each row is the
    sum of the second column over all rows with Read_ID equal to the first column. That step was
    a real CPU burden in the Pandas implementation and this is much faster.

    This function is set up to be parallelized using thread pools, but in practice that may not add
    a lot of speedup. But it has to be extracted from the LemurRunEnv object in order to be passed
    to the thread pool.

    Args:
        P_tgr (pandas.df): Pandas data frame containing the two columns "Read ID" and "P(r|t)*F(t)".
        nthreads (int): Number of threads to parallelize with.

    Returns:
        P_tgr_sum (pandas.df)
    '''
    # Convert each column to individual numpy arrays
    Read_ID = P_tgr["Read_ID"].to_numpy().astype(np.str_)
    Prgt_Ft = P_tgr["P(r|t)*F(t)"].to_numpy()
    # Sort each of them by Read ID
    Read_ID_as = Read_ID.argsort()
    Read_ID = Read_ID[Read_ID_as]
    Prgt_Ft = Prgt_Ft[Read_ID_as]
    # Compute indices of unique read IDs
    Read_ID_unq, Read_ID_Idx = np.unique(Read_ID, return_index=True)
    # Zip up a list of ReadIDs along with the portions of the df to be computed for each read.
    map_args = list(zip(Read_ID_unq.tolist(), np.split(Prgt_Ft, Read_ID_Idx[1:])))
    thread_pool = Pool(nthreads)
    map_res = thread_pool.starmap(logSumExp_ReadId, map_args)
    thread_pool.close()
    thread_pool.join()
    # Reassemble the result into a pandas DF that can go right back into the EM iteration method
    P_tgr_sum = pd.DataFrame(map_res, columns=["Read_ID","P(r|t)*F(t)"], dtype='O')
    P_tgr_sum['P(r|t)*F(t)'] = P_tgr_sum['P(r|t)*F(t)'].astype(float)
    rid=P_tgr_sum["Read_ID"].to_numpy()
    if not np.all(rid[:-1]<rid[1:]):
        P_tgr_sum.sort_values(["Read_ID"],inplace=True)
    P_tgr_sum.set_index(['Read_ID'], inplace=True)
    return P_tgr_sum

def main():
    run = LemurRunEnv()

    if not run.args.sam_input:
        run.log(f"Starting run of minimap2 at {datetime.datetime.now()}", logging.INFO)
        ts = time.time_ns()
        run.run_minimap2()
        t0 = time.time_ns()
        run.log(f"Finished running minimap2 in {(t0-ts)/1000000.0:.3f} ms", logging.INFO)

    t0 = time.time_ns()
    run.init_taxonomy()
    t1 = time.time_ns()
    run.log(f"Finished loading taxonomy in {(t1-t0)/1000000.0:.3f} ms", logging.INFO)
    
    run.init_F()
    t2 = time.time_ns()
    run.log(f"Finished initializing F in {(t2-t1)/1000000.0:.3f} ms", logging.INFO)

    run.build_alignment_model()
    t3 = time.time_ns()
    run.log(f"Finished building alignment model in {(t3-t2)/1000000.0:.3f} ms", logging.INFO)

    run.build_P_rgs_df()
    t4 = time.time_ns()
    run.log(f"Finished constructing P(r|s) in {(t4-t3)/1000000.0:.3f} ms", logging.INFO)

    run.EM_complete()
    t5 = time.time_ns()
    run.log(f"Finished EM in {(t5-t4)/1000000.0:.3f} ms", logging.INFO)

    run.freq_to_lineage_df()
    run.collapse_rank()

    if not run.args.keep_alignments:
        os.remove(run.sam_path)

    exit(0)


if __name__ == "__main__":
    main()
